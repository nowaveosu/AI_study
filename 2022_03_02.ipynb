{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a181b546-619a-44bc-b862-b5d980a2228a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### linear regression 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8d726e-03a4-4970-b7a1-e83ecad234a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#선형회귀 5단계\n",
    "# 슬라이싱을 활용하여 데이터를 x와 t로 분리해야함\n",
    "# W , b = numpy.random.rand(...) \n",
    "# y = numpy.dot(X,W) + b 행렬곱으로 함\n",
    "# 학습률 learning_rate = 1e-3 등등으로 잡아줌, 이상으로 선형회귀 준비완료\n",
    "# f = lambda x : loss func(...)\n",
    "# for step in range(6000) 6000은 임의값\n",
    "# W -= learning_rate * numerical_derivative(f,W)\n",
    "# b -= learning_rate * numerical_derivative(f,b) 수치미분1 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3639282f-6964-451c-824c-bd8577bc030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, t, W모두 행렬로 나타내어 행렬곱 이용하면 y도 행렬로 쉽게 나타남\n",
    "# 예) 입력 1,2,3,4,5 정답 2,3,4,5,6 인경우 W= 1, b = 1이 나올까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b762c3-e122-4d48-b92e-07a60f9f19b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 학습데이터 준비\n",
    "import numpy as np\n",
    "x_data = np.array([1,2,3,4,5]).reshape(5,1)\n",
    "t_data = np.array([2,3,4,5,6]).reshape(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67342bf-f47f-4b20-872a-d50d37dd83d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.12044347]] W.shape = (1, 1) , b=  [0.9824495] ,b.shape =  (1,)\n"
     ]
    }
   ],
   "source": [
    "# 2. 임의의 직선 y = Wx+b정의 \n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)\n",
    "print(\"W = \", W, \"W.shape =\", W.shape,\", b= \",b,\",b.shape = \",b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1bc01a-8139-433a-8a5b-9842a4f91185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 손실함수 E 정의\n",
    "def loss_func(x, t):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return (np.sum((t-y)**2))/(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c12e45-da75-44c1-83ac-0dfed003849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 수치미분 numerical_derivative 및 utility함수 정의\n",
    "def numerical_derivative(f, x): #f는 다변수함수, x는 하나이상의 입력변수 모두 ex(x,y)\n",
    "    delta_x = 1e-4 \n",
    "    grad = np.zeros_like(x) #grad변수는 수치미분값을 저장할것임 \n",
    "    \n",
    "    it = np.nditer(x, flags = ['multi_index'], op_flags = ['readwrite'])\n",
    "    #it는 넘파이의 모든 요소를 가리키는 iterator이다\n",
    "    \n",
    "    while not it.finished: #변수개수만큼 반복\n",
    "        idx = it.multi_index\n",
    "        \n",
    "        tmp_val = x[idx]   #numpy타입은 mutable이므로 원본값 보관\n",
    "        x[idx] = float(tmp_val) + delta_x   #히나의 변수에 대한 수치미분 계산\n",
    "        fx1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x)\n",
    "        grad[idx] = (fx1-fx2) / (2*delta_x) #편미분결과값 grad에 저장\n",
    "        \n",
    "        x[idx] = tmp_val  #x에 원본값 전달\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad #결과값 리턴\n",
    "\n",
    "#손실함수값 계산함수\n",
    "#입력변수 x, t: numpy type\n",
    "def error_val(x, t):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return (np.sum((t-y)**2))/(len(x))\n",
    "#학습을 마친후 임의의 데이터에 대해 미래값 예측하는 함수\n",
    "#입력변수 x:numpy type\n",
    "def predict(x):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67040b37-2b93-466f-acbf-19d81c121a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial error value =  6.2525115347817396e-27 initial W =  [[1.]] Wn ,b = [1.]\n",
      "step =  0 err value =  6.2175255536351874e-27 W =  [[1.]] , b =  [1.]\n",
      "step =  400 err value =  4.25669344457258e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  800 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  1200 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  1600 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  2000 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  2400 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  2800 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  3200 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  3600 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  4000 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  4400 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  4800 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  5200 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  5600 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  6000 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  6400 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  6800 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  7200 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  7600 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n",
      "step =  8000 err value =  2.14412394039071e-28 W =  [[1.]] , b =  [1.]\n"
     ]
    }
   ],
   "source": [
    "#5. 학습율 초기화 및 손싷함수가 최소가 될때까지 W,b업데이트\n",
    "learning_rate = 1e-2 #발산하면 1e-3~6 등으로 바꿔보자\n",
    "\n",
    "f = lambda x : loss_func(x_data,t_data)\n",
    "print(\"initial error value = \", error_val(x_data,t_data), \"initial W = \", W, \"Wn\", \",b =\", b )\n",
    "for step in range(8001):\n",
    "    W -= learning_rate * numerical_derivative(f,W)\n",
    "    b -= learning_rate * numerical_derivative(f,b) \n",
    "    \n",
    "    if (step % 400 == 0):\n",
    "        print(\"step = \", step, \"err value = \", error_val(x_data, t_data), \"W = \", W, \", b = \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa5f712-7848-4f6e-b904-27f4ca8b7789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#손실함수값이 줄어들면서 W, b가 1에 수렴하는걸 알수있다\n",
    "#이상 입력변수 하나의 선형회귀를 알아보았다\n",
    "predict(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192a0ebe-5279-490a-81aa-4262bc48ee8a",
   "metadata": {},
   "source": [
    "#### multi-variable regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e993dc4-5035-4912-89f8-58c4378d62ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1, x2, x3은 국어, 영어, 수학, t 는 총수능점수\n",
    "# 가중치도 3개여야함 x1W1 + x2W2 + x3W3 +b = y 꼴로 나온다 \n",
    "# 25*4가 트레이닝데이터일때 X*W+b - Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60001faa-6de5-4a0c-bfe9-1d0cae267519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 학습데이터 준비\n",
    "import numpy as np\n",
    "\n",
    "loaded_data = np.loadtxt('./data/data-01-test-score.csv', delimiter=',',dtype=np.float32)\n",
    "\n",
    "x_data = loaded_data[:,0:-1] #1~3열을 슬라이싱\n",
    "t_data = loaded_data[:,[-1]] #마지막 4열 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b418ea7-3f3a-4503-8efb-74469fa084a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.7330818 ]\n",
      " [0.34039626]\n",
      " [0.75852446]] W.shape =  (3, 1) , b =  [0.13235444] , b.shape =  (1,)\n"
     ]
    }
   ],
   "source": [
    "# 2. 임의의 직선 y=w1x1 + w2x2 + w3x3 + b 정의\n",
    "W = np.random.rand(3,1) #3x1행렬\n",
    "b = np.random.rand(1)\n",
    "print(\"W = \", W, \"W.shape = \", W.shape, \", b = \", b, \", b.shape = \", b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bdf1102-e1ba-4a36-b449-4a31f10bb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 손실함수 E 정의\n",
    "def loss_func(x, t):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return ( np.sum( (t-y)**2 ) ) / ( len(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da8191a-28f1-4401-a7b1-e29fe135bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 수치미분 numerical_derivative 및 utility함수 정의\n",
    "def numerical_derivative(f, x): #f는 다변수함수, x는 하나이상의 입력변수 모두 ex(x,y)\n",
    "    delta_x = 1e-4 \n",
    "    grad = np.zeros_like(x) #grad변수는 수치미분값을 저장할것임 \n",
    "    \n",
    "    it = np.nditer(x, flags = ['multi_index'], op_flags = ['readwrite'])\n",
    "    #it는 넘파이의 모든 요소를 가리키는 iterator이다\n",
    "    \n",
    "    while not it.finished: #변수개수만큼 반복\n",
    "        idx = it.multi_index\n",
    "        \n",
    "        tmp_val = x[idx]   #numpy타입은 mutable이므로 원본값 보관\n",
    "        x[idx] = float(tmp_val) + delta_x   #히나의 변수에 대한 수치미분 계산\n",
    "        fx1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x)\n",
    "        grad[idx] = (fx1-fx2) / (2*delta_x) #편미분결과값 grad에 저장\n",
    "        \n",
    "        x[idx] = tmp_val  #x에 원본값 전달\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad #결과값 리턴\n",
    "\n",
    "#손실함수값 계산함수\n",
    "#입력변수 x, t: numpy type\n",
    "def error_val(x, t):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return (np.sum((t-y)**2))/(len(x))\n",
    "#학습을 마친후 임의의 데이터에 대해 미래값 예측하는 함수\n",
    "#입력변수 x:numpy type\n",
    "def predict(x):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31c0769d-ea12-4023-bca2-dab50bbabc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial error value =  6.144111293955235 initial W =  [[0.39166055]\n",
      " [0.51234876]\n",
      " [1.10856969]] Wn ,b = [0.1185795]\n",
      "step =  0 err value =  6.144099175744743 W =  [[0.39165163]\n",
      " [0.51235295]\n",
      " [1.10857429]] , b =  [0.11857784]\n",
      "step =  400 err value =  6.139697233118136 W =  [[0.38825426]\n",
      " [0.51395318]\n",
      " [1.11032032]] , b =  [0.11791142]\n",
      "step =  800 err value =  6.136073883038511 W =  [[0.38518085]\n",
      " [0.51541469]\n",
      " [1.11188709]] , b =  [0.11724305]\n",
      "step =  1200 err value =  6.133087352444113 W =  [[0.38240044]\n",
      " [0.51674851]\n",
      " [1.11329392]] , b =  [0.11657294]\n",
      "step =  1600 err value =  6.130621767783969 W =  [[0.37988505]\n",
      " [0.51796496]\n",
      " [1.11455788]] , b =  [0.11590127]\n",
      "step =  2000 err value =  6.128582398899623 W =  [[0.37760939]\n",
      " [0.5190737 ]\n",
      " [1.11569414]] , b =  [0.11522821]\n",
      "step =  2400 err value =  6.126891783646626 W =  [[0.37555058]\n",
      " [0.52008369]\n",
      " [1.11671617]] , b =  [0.11455392]\n",
      "step =  2800 err value =  6.125486567992923 W =  [[0.37368792]\n",
      " [0.52100326]\n",
      " [1.11763593]] , b =  [0.11387852]\n",
      "step =  3200 err value =  6.1243149279757185 W =  [[0.37200271]\n",
      " [0.52184013]\n",
      " [1.11846407]] , b =  [0.11320215]\n",
      "step =  3600 err value =  6.123334465300946 W =  [[0.37047802]\n",
      " [0.52260141]\n",
      " [1.11921007]] , b =  [0.1125249]\n",
      "step =  4000 err value =  6.122510488811571 W =  [[0.36909855]\n",
      " [0.52329367]\n",
      " [1.11988241]] , b =  [0.11184688]\n",
      "step =  4400 err value =  6.121814610540223 W =  [[0.36785045]\n",
      " [0.52392295]\n",
      " [1.12048862]] , b =  [0.11116816]\n",
      "step =  4800 err value =  6.1212235983871 W =  [[0.3667212 ]\n",
      " [0.5244948 ]\n",
      " [1.12103545]] , b =  [0.11048884]\n",
      "step =  5200 err value =  6.120718438253428 W =  [[0.36569948]\n",
      " [0.52501433]\n",
      " [1.12152893]] , b =  [0.10980899]\n",
      "step =  5600 err value =  6.1202835672080935 W =  [[0.36477503]\n",
      " [0.52548619]\n",
      " [1.12197444]] , b =  [0.10912865]\n",
      "step =  6000 err value =  6.119906246368361 W =  [[0.3639386 ]\n",
      " [0.52591468]\n",
      " [1.12237682]] , b =  [0.1084479]\n",
      "step =  6400 err value =  6.119576047948543 W =  [[0.36318178]\n",
      " [0.52630369]\n",
      " [1.12274039]] , b =  [0.10776678]\n",
      "step =  6800 err value =  6.119284435628803 W =  [[0.36249701]\n",
      " [0.52665681]\n",
      " [1.12306902]] , b =  [0.10708534]\n",
      "step =  7200 err value =  6.119024421221993 W =  [[0.36187742]\n",
      " [0.5269773 ]\n",
      " [1.1233662 ]] , b =  [0.10640362]\n",
      "step =  7600 err value =  6.118790283734639 W =  [[0.3613168 ]\n",
      " [0.52726813]\n",
      " [1.12363505]] , b =  [0.10572166]\n",
      "step =  8000 err value =  6.1185773394610745 W =  [[0.36080954]\n",
      " [0.52753202]\n",
      " [1.12387838]] , b =  [0.10503949]\n",
      "step =  8400 err value =  6.118381753825588 W =  [[0.36035056]\n",
      " [0.52777145]\n",
      " [1.12409869]] , b =  [0.10435714]\n",
      "step =  8800 err value =  6.118200387379989 W =  [[0.35993525]\n",
      " [0.52798867]\n",
      " [1.12429826]] , b =  [0.10367464]\n",
      "step =  9200 err value =  6.118030669750169 W =  [[0.35955946]\n",
      " [0.52818573]\n",
      " [1.12447913]] , b =  [0.10299202]\n",
      "step =  9600 err value =  6.117870496452863 W =  [[0.35921944]\n",
      " [0.52836449]\n",
      " [1.12464312]] , b =  [0.10230929]\n",
      "step =  10000 err value =  6.117718144430349 W =  [[0.35891177]\n",
      " [0.52852665]\n",
      " [1.12479189]] , b =  [0.10162648]\n"
     ]
    }
   ],
   "source": [
    "# 5. 학습율 초기화 및 손실함수가 최소가 될때까지 W,b 업데이트\n",
    "learning_rate = 1e-5\n",
    "\n",
    "f = lambda x : loss_func(x_data,t_data)\n",
    "print(\"initial error value = \", error_val(x_data,t_data), \"initial W = \", W, \"Wn\", \",b =\", b )\n",
    "for step in range(10001):\n",
    "    W -= learning_rate * numerical_derivative(f,W)\n",
    "    b -= learning_rate * numerical_derivative(f,b) \n",
    "    \n",
    "    if (step % 400 == 0):\n",
    "        print(\"step = \", step, \"err value = \", error_val(x_data, t_data), \"W = \", W, \", b = \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf37e3f2-161f-42bb-bbf7-a6851c8d2283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([178.89655788])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#손실함수값이 지속적으로 작아지면서 손실함수가 약 6.16을 갖는단걸 알게됨\n",
    "# new값으로 예측해보자\n",
    "test_data = np.array([100,98,81])\n",
    "predict(test_data) #국, 영, 수를 위로 받는 학생의 수능점수 예측값"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
